# SIMD Matrix Multiplication in x86_64 Assembly

[![Architecture](https://img.shields.io/badge/Architecture-x86__64-blue.svg)]()
[![AVX2](https://img.shields.io/badge/SIMD-AVX2-green.svg)]()
[![AVX512](https://img.shields.io/badge/SIMD-AVX512-orange.svg)]()

High-performance matrix multiplication implementations using **AVX2** and **AVX512** SIMD instructions in x86_64 assembly language. This project is designed to be educational, with extensive documentation explaining every aspect of SIMD programming.

## ğŸ¯ Overview

Matrix multiplication is a fundamental operation in:
- Machine Learning & Deep Learning
- Computer Graphics & 3D Rendering
- Scientific Computing & Simulations
- Signal Processing

By leveraging SIMD (Single Instruction, Multiple Data) instructions, we can achieve significant performance improvements over scalar implementations:

| Implementation | Operations per Instruction | Theoretical Speedup |
|----------------|---------------------------|---------------------|
| Scalar (x87)   | 1 float                   | 1x (baseline)       |
| SSE            | 4 floats                  | 4x                  |
| **AVX2**       | **8 floats**              | **8x**              |
| **AVX512**     | **16 floats**             | **16x**             |

## ğŸ“š Documentation

Detailed documentation is available in the `docs/` directory:

| Document | Description |
|----------|-------------|
| [SIMD_CONCEPTS.md](docs/SIMD_CONCEPTS.md) | Fundamentals of SIMD programming |
| [AVX2_REFERENCE.md](docs/AVX2_REFERENCE.md) | Complete AVX2 instruction reference |
| [AVX512_REFERENCE.md](docs/AVX512_REFERENCE.md) | Complete AVX512 instruction reference |
| [MATRIX_MULT_ALGORITHM.md](docs/MATRIX_MULT_ALGORITHM.md) | Algorithm explanation & optimization |
| [**Interactive Visualization**](docs/simd_visualization.html) | **Browser-based animation of the algorithm** |
| [ASSEMBLY_GUIDE.md](docs/ASSEMBLY_GUIDE.md) | **Beginner's guide to x86-64 assembly & SIMD instructions** |

## ğŸ› ï¸ Prerequisites

### Assembler
- **NASM** (Netwide Assembler) version 2.14 or later
  - Windows: Download from [nasm.us](https://www.nasm.us/)
  - Linux: `sudo apt install nasm` or `sudo pacman -S nasm`
  - macOS: `brew install nasm`

### C Compiler (for test harness)
- **Windows**: MinGW-w64 (GCC) or Microsoft Visual C++
- **Linux/macOS**: GCC or Clang

### CPU Requirements
- **AVX2**: Intel Haswell (2013) or AMD Excavator (2015) or later
- **AVX512**: Intel Skylake-X (2017) or AMD Zen 4 (2022) or later

Check your CPU support:
```bash
# Linux
cat /proc/cpuinfo | grep -E "avx2|avx512"

# Windows (PowerShell)
Get-WmiObject -Class Win32_Processor | Select-Object Name, Description
# Or use CPU-Z
```

## ğŸ“ Project Structure

```
simd/
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SIMD_CONCEPTS.md           # SIMD fundamentals
â”‚   â”œâ”€â”€ AVX2_REFERENCE.md          # AVX2 instruction guide
â”‚   â”œâ”€â”€ AVX512_REFERENCE.md        # AVX512 instruction guide
â”‚   â””â”€â”€ MATRIX_MULT_ALGORITHM.md   # Algorithm deep-dive
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ avx2/
â”‚   â”‚   â””â”€â”€ matrix_mult_avx2.asm   # AVX2 implementation
â”‚   â”œâ”€â”€ avx512/
â”‚   â”‚   â””â”€â”€ matrix_mult_avx512.asm # AVX512 implementation
â”‚   â””â”€â”€ include/
â”‚       â””â”€â”€ matrix_mult.h          # C header file
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_main.c                # Test & verification
â”‚   â””â”€â”€ benchmark.c                # Performance benchmarks
â”œâ”€â”€ Makefile                       # Linux/macOS build
â””â”€â”€ build.bat                      # Windows build script
```

## ğŸ”¨ Building

### Linux/macOS

```bash
# Build everything
make all

# Build only AVX2 version
make avx2

# Build only AVX512 version
make avx512

# Run tests
make test

# Clean build artifacts
make clean
```

### Windows (MinGW)

```batch
# Build everything
build.bat

# Build specific version
build.bat avx2
build.bat avx512

# Run tests
build.bat test
```

### Windows (MSVC)

```batch
# Open Visual Studio Developer Command Prompt first
build_msvc.bat
```

## ğŸš€ Usage

### C API

```c
#include "matrix_mult.h"

// Allocate aligned matrices (32-byte alignment for AVX2)
float* A = aligned_alloc(32, M * K * sizeof(float));
float* B = aligned_alloc(32, K * N * sizeof(float));
float* C = aligned_alloc(32, M * N * sizeof(float));

// Initialize matrices...

// Perform matrix multiplication using AVX2
matrix_mult_avx2(A, B, C, M, N, K);

// Or use AVX512 (if available)
matrix_mult_avx512(A, B, C, M, N, K);
```

### Function Signatures

```c
/**
 * Matrix multiplication using AVX2 instructions
 * 
 * @param A     Pointer to matrix A (M x K), row-major, 32-byte aligned
 * @param B     Pointer to matrix B (K x N), row-major, 32-byte aligned
 * @param C     Pointer to result matrix C (M x N), row-major, 32-byte aligned
 * @param M     Number of rows in A and C
 * @param N     Number of columns in B and C
 * @param K     Number of columns in A / rows in B
 */
void matrix_mult_avx2(const float* A, const float* B, float* C,
                      size_t M, size_t N, size_t K);

/**
 * Matrix multiplication using AVX512 instructions
 * (Same parameters as AVX2 version, requires 64-byte alignment)
 */
void matrix_mult_avx512(const float* A, const float* B, float* C,
                        size_t M, size_t N, size_t K);
```

## ğŸ“Š Performance

Expected performance improvements over naive scalar implementation:

| Matrix Size | Scalar | AVX2 | AVX512 | AVX2 Speedup | AVX512 Speedup |
|-------------|--------|------|--------|--------------|----------------|
| 128Ã—128     | ~4ms   | ~0.6ms | ~0.3ms | ~6.5x | ~13x |
| 256Ã—256     | ~32ms  | ~4ms  | ~2ms   | ~8x   | ~16x |
| 512Ã—512     | ~260ms | ~33ms | ~17ms  | ~8x   | ~15x |
| 1024Ã—1024   | ~2.1s  | ~260ms | ~130ms | ~8x | ~16x |

*Benchmarks on Intel Core i9-10900K @ 3.7GHz. Actual results vary by CPU.*

## ğŸ” How It Works

### Key Optimizations

1. **SIMD Parallelism**: Process 8 (AVX2) or 16 (AVX512) floating-point operations per instruction
2. **FMA Instructions**: Fused Multiply-Add reduces latency and improves accuracy
3. **Register Blocking**: Maximize data reuse in vector registers
4. **Cache Optimization**: Access patterns designed for L1/L2 cache efficiency
5. **Loop Unrolling**: Reduce branch overhead and enable instruction-level parallelism

### Register Usage

```
AVX2 (YMM registers - 256 bits each):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ YMM0-YMM7:  Accumulator registers  â”‚  (8 Ã— 8 = 64 floats)
â”‚ YMM8-YMM11: Matrix A elements      â”‚
â”‚ YMM12-YMM15: Matrix B elements     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AVX512 (ZMM registers - 512 bits each):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ZMM0-ZMM15:  Accumulator registers â”‚  (16 Ã— 16 = 256 floats)
â”‚ ZMM16-ZMM23: Matrix A elements     â”‚
â”‚ ZMM24-ZMM31: Matrix B elements     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

The test suite verifies:
- Correctness against naive implementation
- Various matrix sizes (including edge cases)
- Aligned and unaligned memory access
- Numerical accuracy

```bash
# Run all tests
make test

# Expected output:
# Testing AVX2 implementation...
# âœ“ 4x4 matrix: PASSED (max error: 0.000001)
# âœ“ 8x8 matrix: PASSED (max error: 0.000001)
# âœ“ 16x16 matrix: PASSED (max error: 0.000002)
# âœ“ 128x128 matrix: PASSED (max error: 0.000003)
# âœ“ Non-aligned dimensions (17x23): PASSED
# All tests passed!
```

## ğŸ“– Learning Path

If you're new to SIMD programming, we recommend this learning path:

1. **Start with fundamentals**: Read [SIMD_CONCEPTS.md](docs/SIMD_CONCEPTS.md)
2. **Understand the algorithm**: Read [MATRIX_MULT_ALGORITHM.md](docs/MATRIX_MULT_ALGORITHM.md)
3. **Study AVX2 first**: Review [AVX2_REFERENCE.md](docs/AVX2_REFERENCE.md) and the AVX2 assembly code
4. **Progress to AVX512**: Compare with [AVX512_REFERENCE.md](docs/AVX512_REFERENCE.md) and AVX512 code
5. **Experiment**: Modify the code and observe performance changes

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional matrix operations (transpose, inverse)
- ARM NEON implementation
- Further cache optimizations
- GPU comparison benchmarks

## ğŸ“„ License

MIT License - See LICENSE file for details.

## ğŸ™ Acknowledgments

- Intel Intrinsics Guide for instruction documentation
- Agner Fog's optimization manuals
- AMD64 Architecture Programmer's Manual
